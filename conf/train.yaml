### Train Configuration

## Train & Test Parameters

# Note this part set some defaults to Argparser, these can be override by using command line.
#   For example, `python train.py --model_dir ./model_new`

# model_dir: Model base directory to save model, full model directory = model_dir/model_type.
# model_type: `wide` or `deep` or `wide_deep`.
# dynamic train: Bool, a train mode that first take file1 as train data, file2 as test data;
#     then keep training take file2 as train data, file3 as test data ...
# train_data: Train csv data path.
# eval_data: Evaluation small csv data path.
# test_data: Test data path.
# image_train_data: Optional, set empty to not use.
# image_eval_data: Optional, set empty to not use.
# image_test_data: Optional, set empty to not use.

# train_epochs: Train epochs.
# epochs_per_eval: Evaluating test data per `epochs_per_eval` epochs. Only work for non-dynamic mode
# batch_size: Recommend 25600.
# keep_train: Bool, if 0, remove model_dir, else keep training.
# checkpoint_path: Optional, specify the checkpoint path to eval or pred the model. Use latest checkpoint path if not specified.

# pos_sample_loss_weight: Optional, set to use weight_column (weighted loss) for imbalance samples.
#    typically use value neg_sample_num/total_sample_num
# neg_sample_loss_weight: Optional, set to use weight_column (weighted loss) for imbalance samples.
#    typically use value pos_sample_num/total_sample_num
# multivalue: Bool, set 1 or true for features with multiple values.
# num_examples: Approximate value for total samples number, use this value for shuffle buffer size when training.
# num_parallel_calls: Optional, representing the number elements to process in parallel. If not
#    specified, elements will be processed sequentially.
train:
  # path related.
  model_dir: ./model
  model_type: xcross
  train_data: ./data/train.txt
  eval_data: ./data/valid.txt
  test_data: ./data/test.txt
  # train behavior
  train_epochs: 5
  epochs_per_eval: 1
  batch_size: 128
  keep_train: 0
  checkpoint_path:
  shuffle_buffer_size: 10000
  num_parallel_calls: 5


## Runconfig Parameters
# config for `tf.estimator.RunConfig`. See details in https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig
# tf_random_seed: Random seed for TensorFlow initializers. Setting this value allows consistency between reruns.
# save_summary_steps: Save summaries every this many steps. Defaults to 100.
# save_checkpoints_steps: Save checkpoints every this many steps. Can not be specified with save_checkpoints_secs.
# save_checkpoints_secs: Save checkpoints every this many seconds. Can not be specified with save_checkpoints_steps.
#   Defaults to 600 seconds if both save_checkpoints_steps and save_checkpoints_secs are not set in constructor.
#   If both save_checkpoints_steps and save_checkpoints_secs are None, then checkpoints are disabled.
# session_config: a ConfigProto used to set session parameters, or None.
# keep_checkpoint_max: The maximum number of recent checkpoint files to keep. As new files are created, older files are deleted.
#   If None or 0, all checkpoint files are kept. Defaults to 5 (that is, the 5 most recent checkpoint files are kept.)
# keep_checkpoint_every_n_hours: Number of hours between each checkpoint to be saved. The default value of 10,000 hours effectively disables the feature.
# log_step_count_steps: The frequency, in number of global steps, that the global step/sec will be logged during training. Defaults to 100

runconfig:
  tf_random_seed: 123
  save_summary_steps: 1000
  save_checkpoints_steps:
  save_checkpoints_secs: 1800
  keep_checkpoint_max: 5
  keep_checkpoint_every_n_hours: 1
  log_step_count_steps: 1000




